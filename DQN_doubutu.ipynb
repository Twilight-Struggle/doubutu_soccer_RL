{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN_doubutu.ipynb","provenance":[{"file_id":"1za35z6fYFEl7yk30qxFGOyzZDn_6WXQN","timestamp":1597420381081}],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"VoP4dEKpIWbq","colab_type":"text"},"source":["# どうぶつサッカーの環境"]},{"cell_type":"code","metadata":{"id":"ehnX_su0P2KK","colab_type":"code","colab":{}},"source":["from enum import Enum, auto\n","import random\n","import numpy as np\n","import copy\n","\n","BOARD_HEIGHT = 5\n","BOARD_WIDTH = 3\n","\n","\n","class PlayPos(Enum):\n","    FRONTPLAYER = auto()\n","    BACKPLAYER = auto()\n","\n","\n","def playpos_opponent(playpos):\n","    if playpos == PlayPos.FRONTPLAYER:\n","        return PlayPos.BACKPLAYER\n","    elif playpos == PlayPos.BACKPLAYER:\n","        return PlayPos.FRONTPLAYER\n","\n","\n","# move_command = [(x, y, \"さ\")]\n","# kick_command = [[x, y], [a, b]]\n","class Act():\n","    def __init__(self, move, kick):\n","        self.move_command = move\n","        self.kick_command = kick\n","\n","    def is_same(self, aite):\n","        if self.move_command == aite.move_command and self.kick_command == aite.kick_command:\n","            return True\n","        else:\n","            return False\n","\n","\n","def list_to_tuple(_list):\n","    t = ()\n","    for e in _list:\n","        if isinstance(e, list):\n","            t += (list_to_tuple(e), )\n","        else:\n","            t += (e, )\n","    return t\n","\n","\n","def printn(inp):\n","    print(inp, end=\"\")\n","\n","\n","class PieceID(Enum):\n","    SARU_ID = auto()\n","    RISU_ID = auto()\n","    USAGI_ID = auto()\n","    OYASARU_ID = auto()\n","    BALL_ID = auto()\n","\n","\n","class Piece():\n","    def __init__(self, spiece, powe=None):\n","        self.spiece = spiece\n","        self.power = powe\n","        if self.spiece == PieceID.SARU_ID:\n","            self.kick_to = np.array([(1, 1), (1, -1), (-1, 0), (2, 2), (2, -2),\n","                                     (-2, 0)])\n","            if self.power == PlayPos.FRONTPLAYER:\n","                self.identity = \"さ\"\n","            else:\n","                self.identity = \"サ\"\n","        if self.spiece == PieceID.OYASARU_ID:\n","            self.kick_to = np.array([(1, 1), (1, -1), (-1, 0), (2, 2), (2, -2),\n","                                     (-2, 0)])\n","            if self.power == PlayPos.FRONTPLAYER:\n","                self.identity = \"お\"\n","            else:\n","                self.identity = \"オ\"\n","        elif self.spiece == PieceID.RISU_ID:\n","            self.kick_to = np.array([(1, 0), (2, 0), (-1, 0), (-2, 0), (0, 1),\n","                                     (0, 2), (0, -1), (0, -2)])\n","            if self.power == PlayPos.FRONTPLAYER:\n","                self.identity = \"り\"\n","            else:\n","                self.identity = \"リ\"\n","        elif self.spiece == PieceID.USAGI_ID:\n","            self.kick_to = np.array([(1, 0), (2, 0), (0, 1), (0, 2), (0, -1),\n","                                     (0, -2), (1, 1), (2, 2), (1, -1),\n","                                     (2, -2)])\n","            if self.power == PlayPos.FRONTPLAYER:\n","                self.identity = \"う\"\n","            else:\n","                self.identity = \"ウ\"\n","        elif self.spiece == PieceID.BALL_ID:\n","            self.identity = \"ボ\"\n","\n","\n","class Board:\n","    def __init__(self, tn=PlayPos.FRONTPLAYER, cell=None):\n","        self.turn = tn\n","\n","        if cell is None:\n","            self.cells = []\n","            for i in range(BOARD_HEIGHT):\n","                self.cells.append([None for i in range(BOARD_WIDTH)])\n","                reset_flag = True\n","        else:\n","            self.cells = cell\n","            reset_flag = False\n","\n","        self.S_ball = Piece(PieceID.BALL_ID)\n","        self.S_oyasaru_f = Piece(PieceID.OYASARU_ID, PlayPos.FRONTPLAYER)\n","        self.S_saru_f = Piece(PieceID.SARU_ID, PlayPos.FRONTPLAYER)\n","        self.S_risu_f = Piece(PieceID.RISU_ID, PlayPos.FRONTPLAYER)\n","        self.S_usa_f = Piece(PieceID.USAGI_ID, PlayPos.FRONTPLAYER)\n","        self.front_piece = {\n","            \"さ\": self.S_saru_f,\n","            \"り\": self.S_risu_f,\n","            \"う\": self.S_usa_f,\n","            \"お\": self.S_oyasaru_f\n","        }\n","\n","        self.S_oyasaru_s = Piece(PieceID.OYASARU_ID, PlayPos.BACKPLAYER)\n","        self.S_saru_s = Piece(PieceID.SARU_ID, PlayPos.BACKPLAYER)\n","        self.S_risu_s = Piece(PieceID.RISU_ID, PlayPos.BACKPLAYER)\n","        self.S_usa_s = Piece(PieceID.USAGI_ID, PlayPos.BACKPLAYER)\n","        self.back_piece = {\n","            \"サ\": self.S_saru_s,\n","            \"リ\": self.S_risu_s,\n","            \"ウ\": self.S_usa_s,\n","            \"オ\": self.S_oyasaru_s\n","        }\n","        self.reset(reset_flag)\n","\n","    def reset(self, reset_flag=True):\n","        if reset_flag is True:\n","            for i in range(BOARD_HEIGHT):\n","                for j in range(BOARD_WIDTH):\n","                    self.cells[i][j] = None\n","            self.cells[1][1] = self.S_saru_f\n","            self.cells[0][0] = self.S_usa_f\n","            self.cells[0][2] = self.S_risu_f\n","            self.cells[2][1] = self.S_ball\n","            self.cells[3][1] = self.S_saru_s\n","            self.cells[4][0] = self.S_risu_s\n","            self.cells[4][2] = self.S_usa_s\n","            self.turn = PlayPos.FRONTPLAYER if random.random(\n","            ) >= 0.5 else PlayPos.BACKPLAYER\n","        else:\n","            for i in range(BOARD_HEIGHT):\n","                for j in range(BOARD_WIDTH):\n","                    obj = self.cells[i][j]\n","                    if obj is not None:\n","                        if obj.spiece == PieceID.SARU_ID:\n","                            if obj.power == PlayPos.FRONTPLAYER:\n","                                self.cells[i][j] = self.S_saru_f\n","                            else:\n","                                self.cells[i][j] = self.S_saru_s\n","                        elif obj.spiece == PieceID.USAGI_ID:\n","                            if obj.power == PlayPos.FRONTPLAYER:\n","                                self.cells[i][j] = self.S_usa_f\n","                            else:\n","                                self.cells[i][j] = self.S_usa_s\n","                        elif obj.spiece == PieceID.RISU_ID:\n","                            if obj.power == PlayPos.FRONTPLAYER:\n","                                self.cells[i][j] = self.S_risu_f\n","                            else:\n","                                self.cells[i][j] = self.S_risu_s\n","                        elif obj.spiece == PieceID.OYASARU_ID:\n","                            if obj.power == PlayPos.FRONTPLAYER:\n","                                self.cells[i][j] = self.S_oyasaru_f\n","                            else:\n","                                self.cells[i][j] = self.S_oyasaru_s\n","                        elif obj.spiece == PieceID.BALL_ID:\n","                            self.cells[i][j] = self.S_ball\n","\n","    def clone(self):\n","        return Board(self.turn, copy.deepcopy(self.cells))\n","\n","    def display(self):\n","        nums = 4\n","        for side in self.cells[::-1]:\n","            printn(\"{}|\".format(nums))\n","            for cel in side:\n","                if cel is not None:\n","                    printn(cel.identity)\n","                else:\n","                    printn(\"　\")\n","                printn(\"|\")\n","            print()\n","            nums -= 1\n","        print(\".  0  1  2\")\n","\n","    def where_you(self, piece):\n","        arr = np.array(self.cells)\n","        pos = []\n","        for nd in np.where(arr == piece):\n","            if len(nd) != 0:\n","                pos.append(nd[0])\n","        return tuple(pos)\n","\n","    def parse_board_cells(self):\n","        new_cells = []\n","        if self.turn == PlayPos.BACKPLAYER:\n","            piece_dict = {\n","                \"サ\": \"さ\",\n","                \"リ\": \"り\",\n","                \"ウ\": \"う\",\n","                \"オ\": \"お\",\n","                \"さ\": \"サ\",\n","                \"り\": \"リ\",\n","                \"う\": \"ウ\",\n","                \"お\": \"オ\",\n","                \"ボ\": \"ボ\"\n","            }\n","            for i in range(BOARD_HEIGHT):\n","                yokocell = self.cells[BOARD_HEIGHT - 1 - i]\n","                newyokocell = []\n","                for nakami in yokocell[::-1]:\n","                    if nakami is None:\n","                        newyokocell.append(None)\n","                    else:\n","                        newyokocell.append(piece_dict[nakami.identity])\n","                new_cells.append(newyokocell)\n","        else:\n","            for i in range(BOARD_HEIGHT):\n","                newyokocell = []\n","                for j in range(BOARD_WIDTH):\n","                    if self.cells[i][j] is None:\n","                        newyokocell.append(None)\n","                    else:\n","                        newyokocell.append(self.cells[i][j].identity)\n","                new_cells.append(newyokocell)\n","        return new_cells\n","\n","    def tensor_state_parsed(self):\n","        parsed_cells = self.parse_board_cells()\n","        tensored_board = np.empty((9, 5, 3), dtype=np.float32)\n","        piece_list = [\"さ\", \"り\", \"う\", \"お\", \"サ\", \"リ\", \"ウ\", \"オ\", \"ボ\"]\n","        for i in range(9):\n","            for y in range(BOARD_HEIGHT):\n","                for x in range(BOARD_WIDTH):\n","                    if parsed_cells[y][x] == piece_list[i]:\n","                        tensored_board[i][y][x] = 1\n","                    else:\n","                        tensored_board[i][y][x] = 0\n","        return tensored_board\n","\n","    def piece_can_move(self, piece):\n","        my_place = self.where_you(piece)\n","        if len(my_place) == 0:\n","            return None, None\n","        legal_l = []\n","        ball_legal = None\n","        if piece.spiece == PieceID.OYASARU_ID:\n","            for dx in range(-2, 3, 1):\n","                for dy in range(-2, 3, 1):\n","                    new_x = my_place[0] + dx\n","                    new_y = my_place[1] + dy\n","                    if 0 <= new_x <= 4 and 0 <= new_y <= 2:\n","                        if self.cells[new_x][new_y] is None:\n","                            legal_l.append((new_x, new_y))\n","                        elif self.cells[new_x][\n","                                new_y].spiece == PieceID.BALL_ID:\n","                            ball_legal = (new_x, new_y)\n","        else:\n","            for dx in range(-1, 2, 1):\n","                for dy in range(-1, 2, 1):\n","                    new_x = my_place[0] + dx\n","                    new_y = my_place[1] + dy\n","                    if 0 <= new_x <= 4 and 0 <= new_y <= 2:\n","                        if self.cells[new_x][new_y] is None:\n","                            legal_l.append((new_x, new_y))\n","                        elif self.cells[new_x][\n","                                new_y].spiece == PieceID.BALL_ID:\n","                            ball_legal = (new_x, new_y)\n","        return legal_l, ball_legal\n","\n","    def piece_can_kick(self, piece, fromhere, tempboard):\n","        kick_to = piece.kick_to\n","        fromhere = np.array(fromhere)\n","        kick_l = []\n","        if self.turn == PlayPos.BACKPLAYER:\n","            kick_to = -1 * kick_to\n","        for kick in kick_to:\n","            dist = fromhere + kick\n","            if -1 <= dist[0] <= 5 and 0 <= dist[1] <= 2:\n","                if dist[0] == -1 or dist[0] == 5:\n","                    templ = []\n","                    templ.append(list(dist))\n","                    kick_l.append(templ)\n","                elif tempboard[dist[0]][dist[1]] is None:\n","                    templ = []\n","                    templ.append(list(dist))\n","                    kick_l.append(templ)\n","                elif tempboard[dist[0]][dist[1]] == 1:\n","                    pass\n","                elif tempboard[dist[0]][dist[1]].power == self.turn:\n","                    temppiece = tempboard[dist[0]][dist[1]]\n","                    tempboard[dist[0]][dist[1]] = 1\n","                    for oup in self.piece_can_kick(temppiece, tuple(dist),\n","                                                   tempboard):\n","                        templ = []\n","                        templ.append(list(dist))\n","                        tl = templ + oup\n","                        kick_l.append(tl)\n","        return kick_l\n","\n","    # return [Act, Act, Act,...]\n","    def piece_legal_move(self, piece):\n","        acts = []\n","        legal_l, ball_legal = self.piece_can_move(piece)\n","        if legal_l is not None:\n","            for i, spot in enumerate(legal_l):\n","                legal_l[i] = spot + (piece.identity, )\n","            for lem in legal_l:\n","                act = Act(lem, None)\n","                acts.append(act)\n","\n","        if ball_legal is not None:\n","            tempcell = copy.deepcopy(self.cells)\n","            kicker_place = self.where_you(piece)\n","            tempcell[kicker_place[0]][kicker_place[1]] = None\n","            tempcell[ball_legal[0]][ball_legal[1]] = 1\n","            kicks = self.piece_can_kick(piece, ball_legal, tempcell)\n","            ball_legal = ball_legal + (piece.identity, )\n","            for kick in kicks:\n","                kickt = list_to_tuple(kick)\n","                act = Act(ball_legal, kickt)\n","                acts.append(act)\n","        return acts\n","\n","    def legal_moves(self):\n","        piece_dict = self.front_piece if self.turn == PlayPos.FRONTPLAYER else self.back_piece\n","        acts = []\n","        for pie in piece_dict.values():\n","            acts = acts + self.piece_legal_move(pie)\n","        return acts\n","\n","    def parse_legal_moves(self):\n","        parsed_legalmoves = []\n","        if self.turn == PlayPos.BACKPLAYER:\n","            legal = self.legal_moves()\n","            for action in legal:\n","                piece_dict = {\"サ\": \"さ\", \"リ\": \"り\", \"ウ\": \"う\", \"オ\": \"お\"}\n","                new_move_command = [\n","                    BOARD_HEIGHT - 1 - action.move_command[0],\n","                    BOARD_WIDTH - 1 - action.move_command[1],\n","                    piece_dict[action.move_command[2]]\n","                ]\n","                new_kick_commands = None\n","                if action.kick_command is not None:\n","                    new_kick_commands = []\n","                    for kick_to in action.kick_command:\n","                        if kick_to is not None:\n","                            new_kick_to = [\n","                                BOARD_HEIGHT - 1 - kick_to[0],\n","                                BOARD_WIDTH - 1 - kick_to[1]\n","                            ]\n","                        else:\n","                            new_kick_to = None\n","                        new_kick_commands.append(new_kick_to)\n","                new_action = Act(new_move_command, new_kick_commands)\n","                parsed_legalmoves.append(new_action)\n","        else:\n","            parsed_legalmoves = self.legal_moves()\n","        return parsed_legalmoves\n","\n","    # actionのnumはparsedされ、action自体はそのまま\n","    def legalmoves_to_num_parsed(self):\n","        legalmoves_num_act_dict = {}\n","        legalmoves = self.legal_moves()\n","        if not legalmoves:\n","            old_place_num = 15\n","            move_to_num = 15\n","            kick_to_num = 15\n","            action_num = old_place_num * 288 + move_to_num * 18 + kick_to_num\n","            legalmoves_num_act_dict[action_num] = None\n","            return legalmoves_num_act_dict\n","        piece_dict = self.front_piece if self.turn == PlayPos.FRONTPLAYER else self.back_piece\n","        for action in legalmoves:\n","            S_move_ready = piece_dict[action.move_command[2]]\n","            old_place = self.where_you(S_move_ready)\n","            old_place_num = old_place[0] * 3 + old_place[1]\n","            move_to = (action.move_command[0], action.move_command[1])\n","            move_to_num = move_to[0] * 3 + move_to[1]\n","            if action.kick_command is None:\n","                kick_to_num = 15\n","            else:\n","                kick_to = action.kick_command[-1]\n","                if kick_to[0] == -1:\n","                    kick_to_num = 16 if self.turn == PlayPos.FRONTPLAYER else 17\n","                elif kick_to[0] == 5:\n","                    kick_to_num = 17 if self.turn == PlayPos.FRONTPLAYER else 16\n","                else:\n","                    kick_to_num = kick_to[0] * 3 + kick_to[1]\n","            if self.turn == PlayPos.BACKPLAYER:\n","                old_place_num = 14 - old_place_num if 0 <= old_place_num <= 14 else old_place_num\n","                move_to_num = 14 - move_to_num if 0 <= move_to_num <= 14 else move_to_num\n","                kick_to_num = 14 - kick_to_num if 0 <= kick_to_num <= 14 else kick_to_num\n","            action_num = old_place_num * 288 + move_to_num * 18 + kick_to_num\n","            if action_num not in legalmoves_num_act_dict:\n","                legalmoves_num_act_dict[action_num] = action\n","        return legalmoves_num_act_dict\n","\n","    def action_parser(self, action):\n","        if action is None:\n","            if self.turn == PlayPos.FRONTPLAYER:\n","                self.turn = PlayPos.BACKPLAYER\n","            else:\n","                self.turn = PlayPos.FRONTPLAYER\n","            return True, None\n","        legalmoves = self.legal_moves()\n","        existflag = False\n","        for l in legalmoves:\n","            if l.is_same(action):\n","                existflag = True\n","        if not existflag:\n","            return False, None\n","        piece_dict = self.front_piece if self.turn == PlayPos.FRONTPLAYER else self.back_piece\n","        S_move_ready = piece_dict[action.move_command[2]]\n","        move_to = (action.move_command[0], action.move_command[1])\n","\n","        # 移動はaction_parser内で\n","        if action.kick_command is None:\n","            old_place = self.where_you(S_move_ready)\n","            self.cells[old_place[0]][old_place[1]] = None\n","            self.cells[move_to[0]][move_to[1]] = S_move_ready\n","        else:\n","            last_stop = action.kick_command[-1]\n","            if last_stop[0] == -1:\n","                return True, PlayPos.BACKPLAYER\n","            elif last_stop[0] == 5:\n","                return True, PlayPos.FRONTPLAYER\n","            old_place = self.where_you(S_move_ready)\n","            self.cells[old_place[0]][old_place[1]] = None\n","            self.cells[move_to[0]][move_to[1]] = S_move_ready\n","            self.cells[last_stop[0]][last_stop[1]] = self.S_ball\n","\n","        Sarufpos = self.where_you(self.S_saru_f)\n","        if len(Sarufpos) != 0 and Sarufpos[0] == 4:\n","            self.cells[Sarufpos[0]][Sarufpos[1]] = self.S_oyasaru_f\n","        Saruspos = self.where_you(self.S_saru_s)\n","        if len(Saruspos) != 0 and Saruspos[0] == 0:\n","            self.cells[Saruspos[0]][Saruspos[1]] = self.S_oyasaru_s\n","\n","        if self.turn == PlayPos.FRONTPLAYER:\n","            self.turn = PlayPos.BACKPLAYER\n","        else:\n","            self.turn = PlayPos.FRONTPLAYER\n","\n","        return True, None\n","\n","\n","class DQNenv:\n","    def __init__(self):\n","        self.Board = Board()\n","        self.Board.reset()\n","        self.legalmoves_num_act_dict = self.Board.legalmoves_to_num_parsed()\n","\n","    def reset(self):\n","        self.Board.reset()\n","        self.legalmoves_num_act_dict = self.Board.legalmoves_to_num_parsed()\n","\n","    # ボードの状態をnumpy配列で表す。\n","    # board.cell → np.narray\n","    # 下側を自陣、上側を敵陣と固定\n","    # 0ch:自さ, 1ch:自り, 2ch:自う, 3ch:自お,\n","    # 4ch:敵サ, 5ch:敵リ, 6ch:敵ウ, 7ch:敵オ, 8ch:ボ\n","    def tensor_state(self, board_cells):\n","        tensored_board = np.empty((9, 5, 3), dtype=np.float32)\n","        piece_list = [\"さ\", \"り\", \"う\", \"お\", \"サ\", \"リ\", \"ウ\", \"オ\", \"ボ\"]\n","        for i in range(9):\n","            for y in range(BOARD_HEIGHT):\n","                for x in range(BOARD_WIDTH):\n","                    if board_cells[y][x] == piece_list[i]:\n","                        tensored_board[i][y][x] = 1\n","                    else:\n","                        tensored_board[i][y][x] = 0\n","        return tensored_board\n","\n","    def tensor_state_parsed(self):\n","        cells = []\n","        if self.Board.turn == PlayPos.BACKPLAYER:\n","            piece_dict = {\n","                \"サ\": \"さ\",\n","                \"リ\": \"り\",\n","                \"ウ\": \"う\",\n","                \"オ\": \"お\",\n","                \"さ\": \"サ\",\n","                \"り\": \"リ\",\n","                \"う\": \"ウ\",\n","                \"お\": \"オ\",\n","                \"ボ\": \"ボ\"\n","            }\n","            for i in range(BOARD_HEIGHT):\n","                yokocell = self.Board.cells[BOARD_HEIGHT - 1 - i]\n","                newyokocell = []\n","                for nakami in yokocell[::-1]:\n","                    if nakami is None:\n","                        newyokocell.append(None)\n","                    else:\n","                        newyokocell.append(piece_dict[nakami.identity])\n","                cells.append(newyokocell)\n","        else:\n","            for i in range(BOARD_HEIGHT):\n","                newyokocell = []\n","                for j in range(BOARD_WIDTH):\n","                    if self.Board.cells[i][j] is None:\n","                        newyokocell.append(None)\n","                    else:\n","                        newyokocell.append(self.Board.cells[i][j].identity)\n","                cells.append(newyokocell)\n","        return self.tensor_state(cells)\n","\n","    def legalmoves(self):\n","        return self.legalmoves_num_act_dict\n","\n","    def step(self, tensored_action):\n","        now_turn = self.Board.turn\n","        if len(self.legalmoves_num_act_dict) == 0:\n","            action = None\n","        else:\n","            action = self.legalmoves_num_act_dict[tensored_action]\n","        success, winner = self.Board.action_parser(action)\n","        if success is False:\n","            print(\"Irritating input!\")\n","            return None, None, None, None\n","        if winner is not None:\n","            if now_turn == winner:\n","                reward = 1\n","            else:\n","                reward = -1\n","            next_state = None\n","            next_action = None\n","            done = True\n","            skip_turn = False\n","        else:\n","            self.legalmoves_num_act_dict = self.Board.legalmoves_to_num_parsed(\n","            )\n","            reward = 0\n","            next_state = self.tensor_state_parsed()\n","            next_action = list(self.legalmoves_num_act_dict.keys())\n","            done = False\n","            skip_turn = False if len(\n","                self.legalmoves_num_act_dict) != 0 else True\n","\n","        return next_state, next_action, reward, done, skip_turn\n","\n","\n","class BattleEnv:\n","    def __init__(self, frontman, backman):\n","        self.Board = Board()\n","        self.Board.reset()\n","        self.front = frontman\n","        self.back = backman\n","\n","    def progress(self):\n","        while True:\n","            #self.Board.display()\n","            if self.Board.turn == PlayPos.FRONTPLAYER:\n","                now_player = self.front\n","            elif self.Board.turn == PlayPos.BACKPLAYER:\n","                now_player = self.back\n","            while True:\n","                if len(self.Board.legal_moves()) == 0:\n","                    action = None\n","                else:\n","                    action = now_player.action(self.Board)\n","                success, winner = self.Board.action_parser(action)\n","                if success is True:\n","                    break\n","            if winner is not None:\n","                if winner == PlayPos.FRONTPLAYER:\n","                    print(\"Front Player wins!\")\n","                else:\n","                    print(\"Back Player wins!\")\n","                return winner\n","\n","    def reset(self):\n","        self.Board.reset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OMaTRrC4IiP-","colab_type":"text"},"source":["### 必要なI/O\n","環境からの盤面inputは9x5x3であり、必ず下側のプレイヤーが現在の手番プレイヤーとして価値を計算する。\n","\n","outputは4608の手"]},{"cell_type":"markdown","metadata":{"id":"KMLgJ72JI18S","colab_type":"text"},"source":["# DQNの実装"]},{"cell_type":"markdown","metadata":{"id":"20e_kF-sUqcR","colab_type":"text"},"source":["ライブラリのインポートと環境の生成"]},{"cell_type":"code","metadata":{"id":"0Xj947H32e96","colab_type":"code","colab":{}},"source":["import os\n","import datetime\n","import math\n","from collections import namedtuple\n","from itertools import count\n","from tqdm import tqdm_notebook as tqdm\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","env = DQNenv()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B3Jhyn56hI5q","colab_type":"text"},"source":["学習用のリプレイメモリ"]},{"cell_type":"code","metadata":{"id":"k4b936P7hIbq","colab_type":"code","colab":{}},"source":["######################################################################\n","# Replay Memory\n","\n","Transition = namedtuple('Transition',\n","                        ('state', 'action', 'next_state', 'next_actions', 'reward'))\n","\n","\n","class ReplayMemory(object):\n","\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    def push(self, *args):\n","        \"\"\"Saves a transition.\"\"\"\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        self.memory[self.position] = Transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QmU-0B_MmLw7","colab_type":"text"},"source":["ニューラルネットワークの定義。10層の畳み込みニューラルネットワークに全結合層を接続。出力の活性化関数はtanhとして行動価値を-1～1の範囲で出力。\n","\n","In:9ch x 5 x 3\n","\n","Out:16x16x18=4608状態(n // 288:移動元, (n%288) // 18:移動先, (n%288) % 18:蹴り先. 0~14:盤面, 15:パス・何もしない, 16:自分のゴール,17:相手のゴール)"]},{"cell_type":"code","metadata":{"id":"q1XzUzkMtrky","colab_type":"code","colab":{}},"source":["######################################################################\n","# DQN\n","\n","k = 192\n","# fcl_units = 256\n","fcl_units = 4608 * 4\n","class DQN(nn.Module):\n","\n","    def __init__(self):\n","        super(DQN, self).__init__()\n","        self.conv1 = nn.Conv2d(9, k, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(k)\n","        self.conv2 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(k)\n","        self.conv3 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n","        self.bn3 = nn.BatchNorm2d(k)\n","        self.conv4 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n","        self.bn4 = nn.BatchNorm2d(k)\n","        self.conv5 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n","        self.bn5 = nn.BatchNorm2d(k)\n","        self.conv6 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n","        self.bn6 = nn.BatchNorm2d(k)\n","        self.conv7 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n","        self.bn7 = nn.BatchNorm2d(k)\n","        self.conv8 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n","        self.bn8 = nn.BatchNorm2d(k)\n","        self.conv9 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n","        self.bn9 = nn.BatchNorm2d(k)\n","        self.conv10 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n","        self.bn10 = nn.BatchNorm2d(k)\n","        self.fcl1 = nn.Linear(k * 15, fcl_units)\n","        self.fcl2 = nn.Linear(fcl_units, 4608)\n","\n","    def forward(self, x):\n","        x = F.relu(self.bn1(self.conv1(x)))\n","        x = F.relu(self.bn2(self.conv2(x)))\n","        x = F.relu(self.bn3(self.conv3(x)))\n","        x = F.relu(self.bn4(self.conv4(x)))\n","        x = F.relu(self.bn5(self.conv5(x)))\n","        x = F.relu(self.bn6(self.conv6(x)))\n","        x = F.relu(self.bn7(self.conv7(x)))\n","        x = F.relu(self.bn8(self.conv8(x)))\n","        x = F.relu(self.bn9(self.conv9(x)))\n","        x = F.relu(self.bn10(self.conv10(x)))\n","        x = F.relu(self.fcl1(x.view(-1, k * 15)))\n","        x = self.fcl2(x)\n","        return x.tanh()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UZ6k1kGKNWU_","colab_type":"text"},"source":["環境から盤面を受け取りtensorに変換する関数を以下に定義する。どこまで環境側に任せ、どこまでこの関数で変換するかは諸説ある。 ... **環境**"]},{"cell_type":"code","metadata":{"id":"LUAPx1f3PTQh","colab_type":"code","colab":{}},"source":["def get_state(env):\n","    features = np.empty((1, 9, 5, 3), dtype=np.float32)\n","    features[0] = env.tensor_state_parsed()\n","    state = torch.from_numpy(features).to(device)\n","    return state"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LXTN7Gste_Sj","colab_type":"text"},"source":["訓練に使用するハイパーパラメータを設定し、ニューラルネットワーク、オプティマイザ、リプレイメモリを初期化。\n","\n","また、𝜀グリーディー方策で手を選ぶ関数を定義。手は𝜀の確率で、合法手からランダムに選択。それ以外では、方策ネットワーク(policy_net)で行動価値が最大となる手を選択。εは漸減させる。\n","\n","またここで環境に対する入力(=アクション)を決定する。0~4608。\n","\n","合法手リストは[?, ?,..., ?]とする。 ... **環境**"]},{"cell_type":"code","metadata":{"id":"53Uf1NavfIVI","colab_type":"code","colab":{}},"source":["######################################################################\n","# Training\n","\n","BATCH_SIZE = 256\n","GAMMA = 0.99\n","EPS_START = 0.9\n","EPS_END = 0.05\n","EPS_DECAY = 2000\n","OPTIMIZE_PER_EPISODES = 16\n","TARGET_UPDATE = 4\n","\n","policy_net = DQN().to(device)\n","target_net = DQN().to(device)\n","target_net.load_state_dict (policy_net.state_dict())\n","target_net.eval()\n","\n","optimizer = optim.RMSprop(policy_net.parameters(), lr=1e-5)\n","\n","memory = ReplayMemory(131072)\n","\n","def epsilon_greedy(state, legal_moves):\n","    sample = random.random()\n","    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n","        math.exp(-1. * episodes_done / EPS_DECAY)\n","\n","    if sample > eps_threshold:\n","        with torch.no_grad():\n","            q = policy_net(state)\n","            _, select = q[0, legal_moves].max(0)\n","    else:\n","        select = random.randrange(len(legal_moves))\n","    return select\n","\n","def select_action(state, env):\n","    legalmoves_dict = env.legalmoves()\n","    legal_moves = list(legalmoves_dict.keys())\n","\n","    select = epsilon_greedy(state, legal_moves)\n","# [?, ?, ?], tensor([[[?, ?, ?]]])\n","    return legal_moves[select], torch.tensor([[legal_moves[select]]], device=device, dtype=torch.long)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YrWfDaoIu2as","colab_type":"text"},"source":["学習部分"]},{"cell_type":"code","metadata":{"id":"4OLfT9WWu3zd","colab_type":"code","colab":{}},"source":["######################################################################\n","# Training loop\n","\n","losses = []\n","temp = 0\n","def optimize_model():\n","    global temp\n","    temp += 1\n","    if len(memory) < BATCH_SIZE:\n","        return\n","    transitions = memory.sample(BATCH_SIZE)\n","    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n","    # detailed explanation). This converts batch-array of Transitions\n","    # to Transition of batch-arrays.\n","    batch = Transition(*zip(*transitions))\n","\n","    # Compute a mask of non-final states and concatenate the batch elements\n","    # (a final state would've been the one after which simulation ended)\n","    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n","                                          batch.next_state)), device=device, dtype=torch.bool)\n","\n","# batch.next_state ... [?, None, None, ?,...,None]\n","# non_final_mask = [True, False, False, True,...,False]\n","\n","    non_final_next_states = torch.cat([s for s in batch.next_state\n","                                                if s is not None])\n","    state_batch = torch.cat(batch.state)\n","    action_batch = torch.cat(batch.action)\n","    reward_batch = torch.cat(batch.reward)\n","\n","    # 合法手のみ\n","    non_final_next_actions_list = []\n","# batch.next_action\n","# ([3, 5, 7, 9, 10, 32, 33], [4, 17, 32, 37, 41, 42, 49],...,[...])\n","    for next_actions in batch.next_actions:\n","        if next_actions is not None:\n","# non_final_next_actions_list\n","# [[3, 5, 7, 9, 10, 32, 33,..., 0], [4, 17, 32, 37, 41, 42, 49,..., 0],...,[..., 0]]\n","            non_final_next_actions_list.append(next_actions + [next_actions[0]] * (100 - len(next_actions)))\n","    non_final_next_actions = torch.tensor(non_final_next_actions_list, device=device, dtype=torch.long)\n","\n","    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n","    # columns of actions taken. These are the actions which would've been taken\n","    # for each batch state according to policy_net\n","    state_action_values = policy_net(state_batch).gather(1, action_batch)\n","\n","    # Compute V(s_{t+1}) for all next states.\n","    # Expected values of actions for non_final_next_states are computed based\n","    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n","    # This is merged based on the mask, such that we'll have either the expected\n","    # state value or 0 in case the state was final.\n","    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n","    # 合法手のみの最大値\n","    target_q = target_net(non_final_next_states)\n","    # 相手番の価値のため反転する\n","    next_state_values[non_final_mask] = -target_q.gather(1, non_final_next_actions).max(1)[0].detach()\n","    # Compute the expected Q values\n","    expected_state_action_values = next_state_values * GAMMA + reward_batch\n","\n","    # Compute Huber loss\n","    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n","\n","    losses.append(loss.item())\n","\n","    # Optimize the model\n","    optimizer.zero_grad()\n","    loss.backward()\n","    for param in policy_net.parameters():\n","        param.grad.data.clamp_(-1, 1)\n","    optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"62HTi5Sp_UYh","colab_type":"text"},"source":["訓練ループ。𝜀グリーディー方策で手を選択し終局するまで対局を進めます。終局したらまた初期局面から対局を行います。\n","\n","1手進めるごとに経験データをリプレイメモリに格納します。\n","\n","エピソードが終了したタイミングで、OPTIMIZE_PER_EPISODES回に1回の間隔でパラメータの更新を行います。\n","\n","指定のエピソード数に達したら、モデルを保存して終了します。"]},{"cell_type":"code","metadata":{"id":"Vpp3HPHr_YuB","colab_type":"code","colab":{}},"source":["######################################################################\n","# main training loop\n","\n","num_episodes = 10000\n","episodes_done = 0\n","pbar = tqdm(total=num_episodes)\n","for i_episode in range(num_episodes):\n","    # Initialize the environment and state\n","    env.reset()\n","    state = get_state(env)\n","    \n","    for t in count():\n","        # Select and perform an action\n","        move, action = select_action(state, env)\n","        next_state, next_actions, reward, done, skip_turn = env.step(move)\n","\n","        reward = torch.tensor([reward], device=device)\n","\n","        # Observe new state\n","        if done:\n","            next_state = None\n","            next_actions = None\n","        else:\n","          next_state = get_state(env)\n","\n","        # Store the transition in memory\n","        memory.push(state, action, next_state, next_actions, reward)\n","\n","        if done:\n","            break\n","\n","        # Move to the next state\n","        state = next_state\n","\n","    episodes_done += 1\n","    pbar.update()\n","\n","    if i_episode % OPTIMIZE_PER_EPISODES == OPTIMIZE_PER_EPISODES - 1:\n","        # Perform several episodes of the optimization (on the target network)\n","        optimize_model()\n","\n","        # pbar.set_description(f'loss = {losses[-1]:.3e}')\n","\n","        # Update the target network, copying all weights and biases in DQN\n","        if i_episode // OPTIMIZE_PER_EPISODES % TARGET_UPDATE == 0:\n","            target_net.load_state_dict(policy_net.state_dict())\n","\n","modelfile = '/content/drive/My Drive/Colab Notebooks/model.pt'\n","print('save {}'.format(modelfile))\n","torch.save({'state_dict': target_net.state_dict(), 'optimizer': optimizer.state_dict()}, modelfile)\n","\n","print('Complete')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LZRPPZ-MIvUT","colab_type":"code","colab":{}},"source":["plt.plot(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zg-F3W80XHOK","colab_type":"text"},"source":["# 対局"]},{"cell_type":"markdown","metadata":{"id":"uJgl6vH0XgSH","colab_type":"text"},"source":["### Deep Learning"]},{"cell_type":"code","metadata":{"id":"2CgRlVHGXeI3","colab_type":"code","colab":{}},"source":["class GreedyPlayer:\n","    def __init__(self, device):\n","        self.device = device\n","        self.model = DQN().to(device)\n","        checkpoint = torch.load(\"/content/drive/My Drive/Colab Notebooks/model100000.pt\")\n","        self.model.load_state_dict(checkpoint['state_dict'])\n","        self.model.eval()\n","        self.features = np.empty((1, 9, 5, 3), np.float32)\n","\n","    def action(self, Board):\n","        with torch.no_grad():\n","            self.features[0] = Board.tensor_state_parsed()\n","            state = torch.from_numpy(self.features).to(self.device)\n","            q = self.model(state)\n","            # 合法手に絞る\n","            legalmoves_num_act_dict = Board.legalmoves_to_num_parsed()\n","            legal_moves = list(legalmoves_num_act_dict.keys())\n","            next_actions = torch.tensor([legal_moves],\n","                                        device=self.device,\n","                                        dtype=torch.long)\n","            legal_q = q.gather(1, next_actions)\n","            return legalmoves_num_act_dict[legal_moves[legal_q.argmax(\n","                dim=1).item()]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XvONPzdkYwcA","colab_type":"text"},"source":["### ランダムプレイヤー"]},{"cell_type":"code","metadata":{"id":"VBVeWB7_YzUZ","colab_type":"code","colab":{}},"source":["class RandomPlayer:\n","    def __init__(self):\n","        pass\n","\n","    def action(self, Board):\n","        not_lose = []\n","        legalmoves = Board.legal_moves()\n","        legalmoves_p = Board.parse_legal_moves()\n","        for i, action in enumerate(legalmoves_p):\n","            if action.kick_command is None:\n","                not_lose.append(i)\n","            else:\n","                lose_flag = False\n","                last_stop = action.kick_command[-1]\n","                if last_stop[0] == -1:\n","                    lose_flag = True\n","                elif last_stop[0] == 5:\n","                    return legalmoves[i]\n","                if not lose_flag:\n","                    not_lose.append(i)\n","        ret_index = not_lose[random.randrange(len(not_lose))]\n","        return legalmoves[ret_index]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FNveTYQKeyp_","colab_type":"text"},"source":["### ヒューマン"]},{"cell_type":"code","metadata":{"id":"OZw9Vz7Ie0kz","colab_type":"code","colab":{}},"source":["class Human:\n","    def __init__(self):\n","        pass\n","\n","    def action(self, Board):\n","        while True:\n","            legalmoves = Board.legal_moves()\n","            print(\"合法手\")\n","            for i, action in enumerate(legalmoves):\n","                printn(str(i) + '):')\n","                printn(action.move_command)\n","                printn(\" \")\n","                print(action.kick_command)\n","            tmp = input()\n","            try:\n","                inp = int(tmp)\n","                ret = legalmoves[inp]\n","                break\n","            except Exception:\n","                pass\n","        return ret"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hzwTValw0XVn","colab_type":"text"},"source":["## モンテカルロ"]},{"cell_type":"code","metadata":{"id":"9M6MG4D80amH","colab_type":"code","colab":{}},"source":["class MonteCarlo:\n","    def __init__(self):\n","        pass\n","\n","    def policy(self, Board):\n","        not_lose = []\n","        legalmoves = Board.legal_moves()\n","        legalmoves_p = Board.parse_legal_moves()\n","        for i, action in enumerate(legalmoves_p):\n","            if action.kick_command is None:\n","                not_lose.append(i)\n","            else:\n","                lose_flag = False\n","                last_stop = action.kick_command[-1]\n","                if last_stop[0] == -1:\n","                    lose_flag = True\n","                elif last_stop[0] == 5:\n","                    return legalmoves[i]\n","                if not lose_flag:\n","                    not_lose.append(i)\n","        if len(not_lose) != 0:\n","            ret_index = not_lose[random.randrange(len(not_lose))]\n","        else:\n","            # print(\"Error: not_lose is 0\")\n","            # print(legalmoves)\n","            # print(legalmoves_p)\n","            ret_index = 0\n","        return legalmoves[ret_index]\n","\n","    def trial(self, Board, act):\n","        kaisu = 0\n","        tempboard = Board.clone()\n","        myturn = tempboard.turn\n","        success, winner = tempboard.action_parser(act)\n","        if not success:\n","            print(\"act is not recieved\")\n","        if winner is not None:\n","            if winner == myturn:\n","                return 10\n","            else:\n","                return -10\n","        while True:\n","            kaisu += 1\n","            legal_moves_l = tempboard.legal_moves()\n","            # if len(legal_moves_l) == 0:\n","            #     print(\"trial legalmove is 0:kaisu is {}\".format(kaisu))\n","            #     tempboard.display()\n","            #     print(tempboard.turn)\n","            while True:\n","                if len(legal_moves_l) == 0:\n","                    action = None\n","                else:\n","                    action = self.policy(tempboard)\n","                success, winner = tempboard.action_parser(action)\n","                if success is True:\n","                    break\n","                else:\n","                    print(\"fuck\")\n","            if winner is not None:\n","                if winner == myturn:\n","                    return 1\n","                else:\n","                    return -1\n","\n","    def action(self, Board):\n","        scores = {}\n","        n = 50\n","        legalmoves = Board.legal_moves()\n","        for i, act in enumerate(legalmoves):\n","            scores[i] = 0\n","            for j in range(n):\n","                scores[i] += self.trial(Board, act)\n","            scores[i] /= n\n","\n","        max_score = max(scores.values())\n","        for i, v in scores.items():\n","            if v == max_score:\n","                return legalmoves[i]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ev4Xj2_YfUgM","colab_type":"text"},"source":["ランダムと対局"]},{"cell_type":"code","metadata":{"id":"hbJ55vs4fWjA","colab_type":"code","colab":{}},"source":["rand = RandomPlayer()\n","deepl = GreedyPlayer(device)\n","\n","battle = BattleEnv(deepl, rand)\n","\n","while True:\n","    count = 0\n","    for j in range(100):\n","        winner = battle.progress()\n","        if winner == PlayPos.FRONTPLAYER:\n","            count += 1\n","        battle.reset()\n","    won = count\n","    print(\"winrate:{}\".format(won), flush=True)\n","    if won > 80:\n","      break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xyRcBHwIvXgX","colab_type":"text"},"source":["### 人間と対戦"]},{"cell_type":"code","metadata":{"id":"siv1JCn4vY33","colab_type":"code","colab":{}},"source":["human = Human()\n","deepl = GreedyPlayer(device)\n","\n","battle = BattleEnv(deepl, human)\n","winner = battle.progress()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J9NjbaLM0kT5","colab_type":"text"},"source":["### モンテカルロと対戦"]},{"cell_type":"code","metadata":{"id":"_5AaYNMY0oyp","colab_type":"code","colab":{}},"source":["monte = MonteCarlo()\n","deepl = GreedyPlayer(device)\n","\n","battle = BattleEnv(deepl, monte)\n","\n","while True:\n","    count = 0\n","    for j in range(1):\n","        winner = battle.progress()\n","        if winner == PlayPos.FRONTPLAYER:\n","            count += 1\n","        battle.reset()\n","    won = count\n","    print(\"winrate:{}\".format(won), flush=True)\n","    if won > 0:\n","      break"],"execution_count":null,"outputs":[]}]}