# どうぶつサッカーの強化学習
## 参考文献
### コードなどメインで参考にしたもの
https://qiita.com/narisan25/items/e64a5741864d5a3b0db0

### 参考予定文献
- alpha Goに関するまとめ
http://tadaoyamaoka.hatenablog.com/entry/2017/10/20/001735

- alpha Shogiに関するまとめ
http://tadaoyamaoka.hatenablog.com/entry/2017/12/06/210442

- chainerRLで三目並べ
https://qiita.com/uezo/items/87b25c93199d72a56a9a

- KerasRLで三目並べ
https://mahowald.github.io/deep-tictactoe/

- DQNについて
https://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5

- DQNの式がわかりやすい
https://qiita.com/icoxfog417/items/242439ecd1a477ece312

- Kerasの使い方
http://marupeke296.com/IKDADV_DL_No2_Keras.html

## 最初の実装
Windows上でTensorFlow(+Keras)にて実装する。Kerasをカッコ書きにしたのはKerasRLのドキュメントが整っていないためである。
きちんとしたドキュメントが存在するRLライブラリとしてはTensorforceやCoachが挙げられる。

DNNの構成は参考文献と同じく、全結合リンク4つ(=5層のDNN)で最初の3層についてはleaky_reluを適用する。
ロスの計算は平均2乗誤差で計算する。
なお強化学習の場合教師信号は存在しない。このため参考文献ではQ(s,a)値を更新した場合の予測結果を教師信号として最小2乗誤差を取っている。
ただしこの方法だと教師信号として使う期待値側も微分してしまうかもしれない。[参考](https://qiita.com/icoxfog417/items/242439ecd1a477ece312)

大きく変更が必要なのが入力及び出力である。
三目並べでは9つの出力でどの手を指すか選ぶことができるが、どうぶつサッカーではアクションの数が膨大であり、列挙することは難しい。
このため盤面ごとの価値ををスカラーで出力することによる。
現在の状況(s)と可能な1手(a)は環境から与えられる。これにより次の盤面の状況を得ることができる。
次の盤面の状況をそれぞれ評価することで次の1手を選ばせることとする。
この出力を実現するためには、入力として現在の盤面と現在の手番プレイヤーを入力する必要がある。
またそれぞれのコマ(7種類)がどこに存在するかはone-hot表現で示す。
以上より5 × 3 × 9のテンソルに先手後手を示す1つのスカラーを入力とする。

このようなアーキテクチャを用いる場合、学習が必要なのはQ(s,a)ではなくQ(s)になる。
自分のターンをQ(), 相手ターンをQ'()と表現する。
```
Q(s)→Q'(s1) →Q(s3)
            →Q(s4)
    →Q'(s2) →Q(s5)
            →Q(s6)
```

一般に、Q学習は今の状態を`s`, 実行するアクションを`a`として次のように表される。
```
Q(s,a) = Q(s,a) + alpha (reward + gammma* max{Q(s',a')}- Q(s,a)) …(1)
```
今回の実装ではQ(s)を学習しなくてはならない。
例えば上記の例でQ(s)について学習させるとすると
```
Q(s) = Q(s) + alpha (reward + gamma * max{Q'(s1), Q'(s2)} - Q(s)) …(2)
```
となる。Q'(s1)のほうが値が大きかったと仮定すると、エージェントはQ'(s1)に遷移するようにアクションを実行する。
このアクションの直後、(2)によってQ(s)を更新する。
その後相手プレイヤーがアクションを行い、例えばQ(s4)の状態に遷移したものとする。
この場合、Q'(s1)は次のような更新式をもつ。
```
Q'(s1) = Q'(s1) + alpha (reward + gamma * Q(s4) - Q'(s1))
```
以上の2つの式がQ(s)の更新式になる。

## メモリ使いすぎ問題
最初、行動価値関数で実装していたがそれだとメモリの使用数が多すぎてうまく動作しなかった。
このため上述のように状態価値関数を使用する仕様に変更した。
ここで更に確認しなくてはならないこととして、keyとして保存される盤面のバイト数がある。
これが大きすぎるとメモリに乗り切らない。
このゲームの状態数は15C7 * 7! * 2 = 64864800であり、keyのバイト数が100であれば6GBほど、1000であれば60GBほどメモリを消費する。
